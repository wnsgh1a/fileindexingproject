import os
import shutil
import hashlib
import difflib
from datetime import datetime
from collections import defaultdict, deque
from docx import Document  # ì¶”ê°€: .docx í…ìŠ¤íŠ¸ ì¶”ì¶œìš©

# âœ… ì‚­ì œ í›„ë³´êµ° ë””ë ‰í† ë¦¬ ê²½ë¡œ
CANDIDATE_DIR = r"C:\Users\qazws\Desktop\ì‚­ì œí›„ë³´"
GUBOJEON_DIR = os.path.join(CANDIDATE_DIR, "êµ¬ë²„ì „")
DUPLICATE_DIR = os.path.join(CANDIDATE_DIR, "ì¤‘ë³µíŒŒì¼")

# âœ… í•´ì‹œê°’ ê³„ì‚° (ì „ì²´ íŒŒì¼ í•´ì‹œ)
def calculate_file_hash(file_path):
    sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            sha256.update(f.read())  # ì „ì²´ íŒŒì¼ í•´ì‹œ ê³„ì‚°
        return sha256.hexdigest()
    except Exception as e:
        print(f"Error hashing {file_path}: {e}")
        return None

# âœ… íŒŒì¼ ì´ë™ í•¨ìˆ˜
def move_to_category(file_path, category, reason=""):
    category_dir = os.path.join(CANDIDATE_DIR, category)
    os.makedirs(category_dir, exist_ok=True)

    destination = os.path.join(category_dir, os.path.basename(file_path))
    base, ext = os.path.splitext(destination)
    counter = 1

    while os.path.exists(destination):
        destination = f"{base}_{counter}{ext}"
        counter += 1

    shutil.move(file_path, destination)
    print(f"âœ… {os.path.basename(file_path)} â†’ {category_dir} ì´ìœ : {reason}")

# âœ… íŒŒì¼ëª… ì •ì œ í•¨ìˆ˜
def simplify_filename(filename):
    import re
    name, _ = os.path.splitext(filename.lower())
    # ë²„ì „ íŒ¨í„´ ì œê±° (v2, ver3, v_4, ver.5 ë“±)
    name = re.sub(r"(ver|v)?[\._\-]?[0-9]+(\.[0-9]+)?", "", name)
    keywords = ["rev", "íŒë³¸", "draft", "ìˆ˜ì •ë³¸", "ìµœì¢…", "ë³µì‚¬ë³¸"]
    for keyword in keywords:
        name = name.replace(keyword, "")
    return name.strip().replace("_", "").replace(" ", "")

# âœ… .docx ë‚´ìš© ì¶”ì¶œ í•¨ìˆ˜
def extract_docx_text(path):
    try:
        doc = Document(path)
        return "\n".join([p.text for p in doc.paragraphs])
    except:
        return ""

# âœ… í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ í•¨ìˆ˜ (.docx ì „ìš©)
def is_content_similar(file1, file2, threshold=0.85):
    try:
        text1 = extract_docx_text(file1)
        text2 = extract_docx_text(file2)
        similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
        return similarity >= threshold
    except:
        return False

# âœ… ìœ ì‚¬ë„ ê¸°ë°˜ íŒŒì¼ í´ëŸ¬ìŠ¤í„° êµ¬ì„± í•¨ìˆ˜
def build_similarity_clusters(similarity_groups):
    visited = set()
    clusters = []

    for file in similarity_groups:
        if file in visited:
            continue
        cluster = set()
        queue = deque([file])
        while queue:
            current = queue.popleft()
            if current in visited:
                continue
            visited.add(current)
            cluster.add(current)
            queue.extend(similarity_groups[current] - visited)
        if cluster:
            clusters.append(cluster)
    return clusters

# âœ… ì „ì²´ ê²€ì‚¬ ê¸°ë°˜ ì •ë¦¬ í•¨ìˆ˜ (ì¤‘ë³µ + êµ¬ë²„ì „)
def isolate_all(directory):
    print(f"\nğŸ“Œ ì „ì²´ í´ë” ê¸°ë°˜ ì¤‘ë³µ ë° êµ¬ë²„ì „ ì •ë¦¬ ì‹œì‘: {directory}\n")

    file_paths = []
    file_hashes = defaultdict(list)
    file_groups = defaultdict(list)
    duplicate_hashes = set()

    # âœ… í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ íŒŒì¼ ìˆ˜ì§‘
    for root, _, files in os.walk(directory):
        for file in files:
            path = os.path.join(root, file)
            if not os.path.isfile(path):
                continue

            file_paths.append(path)
            file_hash = calculate_file_hash(path)
            if file_hash:
                file_hashes[file_hash].append(path)

    # 2. ì¤‘ë³µ ì •ë¦¬ (í•´ì‹œê°€ ê°™ìœ¼ë©´ ë¬´ì¡°ê±´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)
    for hash_value, hash_group in file_hashes.items():
        if len(hash_group) <= 1:
            continue
        duplicate_hashes.update(hash_group)
        latest = max(hash_group, key=lambda x: os.path.getmtime(x))
        for f in hash_group:
            if f != latest:
                move_to_category(f, "ì¤‘ë³µíŒŒì¼", reason="ì „ì²´ ê²€ì‚¬ ê¸°ë°˜ ì¤‘ë³µíŒŒì¼ ì •ë¦¬")

    # 3. êµ¬ë²„ì „ ì •ë¦¬ìš© ê·¸ë£¹í™” (ì¤‘ë³µ íŒŒì¼ ì œì™¸)
    for path in file_paths:
        if not os.path.isfile(path) or path in duplicate_hashes:
            continue
        simplified = simplify_filename(os.path.basename(path))
        file_groups[simplified].append(path)

    # 4. êµ¬ë²„ì „ ì •ë¦¬ (ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ â†’ ìµœì‹ ë§Œ ìœ ì§€)
    for group, files in file_groups.items():
        if len(files) <= 1:
            continue
        similarity_graph = defaultdict(set)
        for i in range(len(files)):
            for j in range(i + 1, len(files)):
                if is_content_similar(files[i], files[j]):
                    similarity_graph[files[i]].add(files[j])
                    similarity_graph[files[j]].add(files[i])

        # ğŸ§© ìœ ì‚¬ë„ê°€ ì—°ê²°ë˜ì§€ ì•Šì€ ë‹¨ë… íŒŒì¼ë„ í¬í•¨ë˜ë„ë¡ ë³´ì™„
        all_related = set()
        for k, v in similarity_graph.items():
            all_related.add(k)
            all_related.update(v)
        unclustered = set(files) - all_related
        for f in unclustered:
            similarity_graph[f] = set()

        clusters = build_similarity_clusters(similarity_graph)
        for cluster in clusters:
            latest = max(cluster, key=lambda x: os.path.getmtime(x))
            for f in cluster:
                if f != latest:
                    move_to_category(f, "êµ¬ë²„ì „", reason="ë‚´ìš© ìœ ì‚¬ ê¸°ë°˜ êµ¬ë²„ì „ ì •ë¦¬")

    print("\nâœ… ì „ì²´ ì •ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")
